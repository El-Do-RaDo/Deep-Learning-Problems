## 1. SVM推导，SVM多分类方法，LR loss function推导，决策树含义

### SVM

* https://en.wikipedia.org/wiki/Support_vector_machine

### LR

* https://en.wikipedia.org/wiki/Logistic_regression

### Decision tree

* https://en.wikipedia.org/wiki/Decision_tree

## 2. 如何解决欠拟合/过拟合

### 欠拟合
* 添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。
* 添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。
* 减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。

### 过拟合

* Early Stopping：在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练
* 数据集扩增：从数据源头采集更多数据、复制原有数据并加上随机噪声、重采样、根据当前数据集估计数据分布参数，使用该分布产生更多数据等
* 采用正则化方法
* Dropout：在训练的时候让神经元以一定的概率不工作

## 3. 正则化方法

* 正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则。
* L0范数是指向量中非0的元素的个数。L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。
* L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。
* L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。它们都是服从均值为0，协方差为$1/\lambda$。当$\lambda = 0$时，即没有先验）没有正则项，则相当于先验分布具有无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练集数据，参数$w$可以变得任意大从而使得模型不稳定，即方差大而偏差小。$\lambda$越大，表明先验分布协方差越小，偏差越大，模型越稳定。即加入正则项是在偏差bias与方差variance之间做平衡tradeoff。

## 4. 常用CNN

### AlexNet

* 成功使用ReLU作为激活函数，并验证起效果在较深的网络超过sigmoid，解决sigmoid在网络较深时的梯度弥散问题
* 使用Dropout随机忽略部分神经元，避免模型过拟合
* 使用重叠的maxpool，避免平均池化的模糊效果；步长比池化核尺寸小，让池化层输出之间产生重叠和覆盖，提升特征的丰富性
* 提出LRN层，对局部神经元活动创建竞争机制，响应更大的值相对更大，抑制反馈较小的神经元
* CUDA加速
* 数据增强

### VGGNet

* 使用`3*3`的卷积核：三个`3*3`卷积核的有效reception field和单个`7*7`的卷积核相同
* 层数更深，更多的非线性变换
* 参数更少
* 时间消耗很大，每个网络需要训练2-3周
* 先训练小网络，再fine-tune到大网络，收敛速度更快

### GoogleNet

* 22层，层数更深
* Inception Module：Network in Network，通过增加输出通道数量来增强卷积层表达能力，但增大计算量并且造成过拟合
* 更少的参数(bottleneck)：使用`1*1`卷积核，减少参数量，减轻过拟合，增加网络非线性表达能力
* 没有全连接层
* 希望靠后的Inception Module可以捕捉更高阶的抽象特征，因此靠后的Inception Module的卷积空间集中度应该逐渐降低，换言之，大面积卷积核占比更大
* 采用辅助分类节点，相当于做模型融合，并且给网络增加了反向传播的梯度信号，也提供了额外的正则化
* BN层给每一个mini-batch数据内部进行normalization，使输出规范到N(0, 1)分布，减少内部神经元分布的改变。BN加速大型神经网络训练，同时大幅提高分类准确率

### ResNet

* Degradation：随着网络深度的增加，准确度饱和并且迅速减少。提出了深度残差学习的概念来解决这一问题
* 152层，层数加深
* 将前一层的输出直接映射到后一层，保护输出的完整性，使得深层的网络表现得至少和浅层网络一样好
* 残差学习单元，将学习目标变为输出和输入的差别，即残差
* 改进的版本中发现前馈和反馈信号可以直接传输，因此skip connection的非线性激活函数替换为Identity Mappings (y = x)，同时每一层都使用batch normalization

### Ref link

* https://zhuanlan.zhihu.com/p/31006686?group_id=915291541967994880

## 5. 调参技巧

### 基本步骤

* 优先调 learning rate：学习速率会很大程度上影响模型的表现。同样的模型采用不同的learning rate 可能会表现迥异。所以凭感觉建好一个模型后如果对表现不满意应该优先调学习速率。模型具有理论容量和有效容量两种能力指标，理论容量是由卷积核数量，模型深度等决定的。但是有效容量会受学习速率影响。
* 加 Dropout， 加 BN，加Data Argument：一般而言，在设计模型之初我们都信奉大力出奇迹。模型尽量深，卷积核尽量多，强行让模型拟合训练集。这时容易遇到的问题就是过拟合。解决过拟合的第一要素是模型的正则化，最有效方法是加大训练数据集。如果有钱，那多标记数据。如果没钱，那就给训练数据加增强，图像裁剪，对称变换，旋转平移，都可以让模型在验证集上的表现更好。此外，增加常用的正则化也可以提升模型的表现。
* 调模型的层数和卷积核数量：走到这一步的时候都是迫不得已了。这两个参数都是对模型理论容量具有巨大影响的参数，一旦调整了之后所有东西都要重新调。增大模型的层数和卷积核的数量都会提升模型的容量。不同的是，增大模型层数（让模型变高）可以让模型获得更好的非线性，模型容量指数增加，缺点是加大层数会让模型更难训练，面临梯度消失的风险，引入死单元。增加卷积核（让模型变胖）可以在不引入训练困难的情况下让模型更好的拟合训练集，也就是降低 training loss，但是会更容易过拟合。

### 三种可能情况（在验证集上）

* 模型表现非常好，在训练集和验证集上都满足我们的目标。那不用调了
* 模型在训练集上的误差很小，且各种（softmax 等等） loss 能减小到一个很小的值（对于我自己而言小于0.01），但是验证集的 loss 相对较大（对我的问题而言，一般会在0.3~0.6）。那就是过拟合了
* 在训练集和验证集上的loss都比较大，都大于0.3，那可能就是欠拟合了

### 可视化工具

* Visualize Layer Activations：比较理想的layer activation应该具备sparse和localized的特点。 如果训练出的模型，用于预测某张图片时，发现在卷积层里的某个feature map的activation matrix可视化以后，基本跟原始输入长得一样，基本就表明出现了一些问题，因为这意味着这个feature map没有学到多少有用的东西
* Visualize Layer Weights：良好的卷积层的weight可视化出来会具备smooth的特性；如果存在很多噪点，则模型训练可能出问题
* Retrieving Images That Maximally Activate a Neuron：对于一个特定的卷积层的Feature Map里的某个神经元，我们可以找到使得这个神经元的activation最大的那些图片，然后再从这个Feature Map neuron还原到原始图片上的receptive field，即可以看到是哪张图片的哪些region maximize了这个neuron的activation；通过寻找maximizing activation某个特定neuron的方法也许并没有真正找到本质的信息。因为即便是对于某一个hidden layer的neurons进行线性加权，也同样会对一组图片表现出相近的semantic亲和性，并且，这个发现在不同的数据集上得到了验证
* Embedding the Hidden Layer Neurons with  t-SNE：通过t-SNE对隐藏层进行降维，然后以降维之后的两维数据分别作为x、y坐标（也可以使用t-SNE将数据降维到三维，将这三维用作x、y、z坐标，进行3d clustering），对数据进行clustering，人工review同一类图片在降维之后的低维空间里是否处于相邻的区域。http://www.datakit.cn/blog/2017/02/05/t_sne_full.html
* Occluding Parts of the Image：对于一张输入图片，使用一个小尺寸的灰度方块图作为掩模，对该原始图片进行遍历掩模，每作一次掩模，计算一下CNN模型对这张掩模后图片的分类预测输出，同时，找到一个在训练集上activation最大的feature map，每作一次掩模，记录下来以掩模图片作为输入数据之后的feature map矩阵，将所有掩模所产生的这些feature map矩阵进行elementwise相加，就可以观察到掩模图片的不同区域对分类预测结果以及feature map的activation value的影响

### Ref link

* https://www.zhihu.com/question/25097993

## 6. KMeans，Adaboost

### KMeans

* 分配(Assignment)：将每个观测分配到聚类中，使得组内平方和（WCSS）达到最小
  因为这一平方和就是平方后的欧氏距离，所以很直观地把观测分配到离它最近得均值点即可:

 <img src="https://latex.codecogs.com/gif.latex?
\large 
\dpi{400}
{S_i}^{(t)} = \{x_p:{||x_p - {m_i}^{(t)}||}^2 \leq {||x_p - {m_j}^{(t)}||}^2 \quad\forall j, 1\leq j \leq k \}"/>

  其中每个$x_p$都只被分配到一个确定的聚类$S^t$中，尽管在理论上它可能被分配到2个或者更多的聚类
* 更新(Update)：计算得到上步得到聚类中每一聚类观测值的图心，作为新的均值点:
  $${m_i}^{(t + 1)} = {1 \over |{S_i}^{(t)}|}{\sum}_{x_j\in{S_i}^{(t)}}x_j$$
  因为算术平均是最小二乘估计，所以这一步同样减小了目标函数组内平方和（WCSS）的值
* 这一算法将在对于观测的分配不再变化时收敛。由于交替进行的两个步骤都会减小目标函数WCSS的值，并且分配方案只有有限种，所以算法一定会收敛于某一（局部）最优解。注意：使用这一算法无法保证得到全局最优解。
这一算法经常被描述为“把观测按照距离分配到最近的聚类”。标准算法的目标函数是组内平方和（WCSS），而且按照“最小二乘和”来分配观测，确实是等价于按照最小欧氏距离来分配观测的。如果使用不同的距离函数来代替（平方）欧氏距离，可能使得算法无法收敛。然而，使用不同的距离函数，也能得到k-均值聚类的其他变体，如球体k-均值算法和k-中心点算法。
* https://en.wikipedia.org/wiki/K-means_clustering

### Adaboost

* AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。AdaBoost方法对于噪声数据和异常数据很敏感。但在一些问题中，AdaBoost方法相对于大多数其它学习算法而言，不会很容易出现过拟合现象。AdaBoost方法中使用的分类器可能很弱（比如出现很大错误率），但只要它的分类效果比随机好一点（比如两类问题分类错误率略小于0.5），就能够改善最终得到的模型。而错误率高于随机分类器的弱分类器也是有用的，因为在最终得到的多个分类器的线性组合中，可以给它们赋予负系数，同样也能提升分类效果。
* https://en.wikipedia.org/wiki/AdaBoost

## 7. LR公式为啥用e

* 指数族分布：http://blog.csdn.net/u011467621/article/details/48197943

## 8. 生成模型和判别模型

* 生成模型需要借助联合概率分布，判别模型不需要联合概率分布

## 9. 样本不均衡问题

* 随机采样：通过上采样和下采样解决，即多的样本通过取其中一部分，少的样本重复利用。但是他们也存在一些问题。对于随机欠采样，由于采样的样本要少于原样本集合，因此会造成一些信息缺失，未被采样的样本往往带有很重要的信息。对于随机过采样，由于需要对少数类样本进行复制因此扩大了数据集，造成模型训练复杂度加大，另一方面也容易造成模型的过拟合问题
* SMOTE算法：对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中
    - 对于少数类中每一个样本$x$，以欧氏距离为标准计算它到少数类样本集$S_{min}$中所有样本的距离，得到其k近邻
    - 根据样本不平衡比例设置一个采样比例以确定采样倍率$N$，对于每一个少数类样本$x$，从其k近邻中随机选择若干个样本，假设选择的近邻为$\hat{x}$
    - 对于每一个随机选出的近邻$\hat{x}$，分别与原样本按照如下的公式构建新的样本: $$x_{new} = x + rand(0,1) \times (\hat{x} - x)$$
    - 由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠(Overlapping)的问题，为了解决SMOTE算法的这一缺点提出一些改进算法，其中的一种是Borderline-SMOTE算法
* Informed Undersampling：主要有两种方法分别是EasyEnsemble算法和BalanceCascade算法
    - EasyEnsemble算法类似于随机森林的Bagging方法，它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本$S_{maj}$，通过n次有放回抽样生成n份子集，少数类样本分别和这n份样本合并训练一个模型，这样可以得到n个模型，最终的模型是这n个模型预测结果的平均值
    - BalanceCascade算法是一种级联算法，BalanceCascade从多数类$S_{maj}$中有效地选择N且满足$N=S_{min}$，将N和$S_{min}$合并为新的数据集进行训练，新训练集对每个多数类样本$x_i$进行预测若预测对则$S_{maj}=S_{maj}-x_i$。依次迭代直到满足某一停止条件，最终的模型是多次迭代模型的组合
* 代价敏感学习：核心要素是代价矩阵。从学习模型出发，可以对学习模型进行改造以适应不平衡数据下对学习；从贝叶斯风险理论出发，可以看作是分类结果的后处理；从预处理的角度出发，可以将代价看作权重调整
* Adacost算法：Adaboost更新策略为正确分类样本权重降低，错误分类样本权重加大，最终的模型是多次迭代模型的一个加权线性组合，分类越准确的分类器将会获得越大的权重。Adacost算法修改了Adaboost算法的权重更新策略，其基本思想是对于代价高的误分类样本大大地提高其权重，而对于代价高的正确分类样本适当地降低其权重，使其权重降低相对较小。总体思想是代价高样本权重增加得大降低得慢
* 评价方法：正确率和F值、G-Mean、ROC曲线和AUC
* 修改loss函数，修改样本权值，让少的样本得到更大的权值
* Reference: http://www.jianshu.com/p/3e8b9f2764c8

## 10. 决策树如何回归

* 让所有节点求平均值

## 11. 拟牛顿法

* 对loss函数进行二阶泰勒展开，让一阶项和二阶项极小化，得到迭代公式

## 12. BP和SGD公式推导

## 13. Dropout如何解决过拟合

* 取平均的作用： 先回到正常的模型（没有dropout），我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。（例如 3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果）。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合
* 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的模式（鲁棒性）。（这个角度看 dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高）

## 14. Batch Normalization的作用

* 在深度网络的训练中，每一层网络的输入都会因为前一层网络参数的变化导致其分布发生改变，这就要求我们必须使用一个很小的学习率和对参数很好的初始化，但是这么做会让训练过程变得慢而且复杂。作者把这种现象称作Internal Covariate Shift。通过Batch Normalization可以很好的解决这个问题
* 对层间的数据做均值和方差的修正，最终它让一些饱和非线性的激活函数可以被使用（如Sigmoid）而这些激活函数的使用正是梯度弥散的罪魁祸首
* Ref Link: https://www.zhihu.com/question/38102762

## 15. CPU和GPU的区别

* CPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理；而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。
* GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。而CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部分。
* CPU基于低时延设计，有强大的ALU，可以在很少的时钟周期内完成算数计算；时钟频率非常高；大量缓存可以使得访问时直接从缓存读取；复杂的逻辑控制单元，提供分支预测能力降低时延；数据转发的逻辑控制单元决定这些指令在pipeline中的位置并且尽可能快的转发一个指令的结果给后续的指令。
* GPU基于大吞吐量设计，GPU的特点是有很多的ALU和很少的cache. 缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram（因为需要访问的数据保存在dram中而不是cache里面），获取数据后cache会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问dram，自然会带来延时的问题；GPU的控制单元（左边黄色区域块）可以把多个的访问合并成少的访问；GPU的虽然有dram延时，却有非常多的ALU和非常多的thread. 为啦平衡内存延时的问题，我们可以中充分利用多的ALU的特性达到一个非常大的吞吐量的效果。尽可能多的分配多的Threads.通常来看GPU ALU会有非常重的pipeline就是因为这样。
* Ref Link: http://blog.csdn.net/witnessai1/article/details/52730702

## 16. CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

* 以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。 
* CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。

## 17. 什么样的资料集不适合用深度学习? 

* 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 
* 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。

## 18. 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法?

* 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。

## 19. 什么造成梯度消失问题? 

* 神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。
* 梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。

## 20. 广义线性模型是怎被应用在深度学习中?

* 深度学习从统计学角度，可以看做递归的广义线性模型。广义线性模型相对于经典的线性模型(`y=wx+b`)，核心在于引入了连接函数`g(.)`，形式变为：`y=g−1(wx+b)`。深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的连接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数

## 21. Weights Initialization. 不同的方式，造成的后果。为什么会造成这样的结果

* lecun_uniform /  glorot_normal / he_normal / batch_normal
* 小随机数初始化：将权重初始化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：`W = 0.01 * np.random.randn(D,H)`。其中randn函数是基于`N(0, 1)`的一个高斯分布来生成随机数的。并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。
* Xavier：使用`1/sqrt(n)`或`2/sqrt(n_in + n_out)`校准方差。小随机数初始化的问题是，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。因此可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。`2/sqrt(n_in + n_out)`是基于妥协和对反向传播中梯度的分析得出的结论。MSRA提出针对ReLU神经元的特殊初始化，认为网络中神经元的方差应该是`2.0/n`，这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。
* 稀疏初始化（Sparse initialization）。另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。
* batch normalization：让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层）与激活函数之间添加一个BatchNorm层。

## 22. 为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？

* 总体思想是，网络层数越深，local optima越接近global optimum，因此可以避开较差的local optima
* Ref link: https://stats.stackexchange.com/questions/203288/understanding-almost-all-local-minimum-have-very-similar-function-value-to-the

## 23. Loss有哪些定义方式（基于什么？）， 有哪些优化方式，怎么优化，各自的好处，以及解释

* 0-1 loss
* Hinge loss (SVM)
* Log loss (Cross-Entropy / K-L散度)
* Square loss (linear regression)
* Exponential loss (boosting)

## 24. Activation Function. 选用什么，有什么好处，为什么会有这样的好处

### Sigmoid

* $$\sigma (x) = 1 / (1 + e^{-x})$$
* 它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和的激活（1）。
* Sigmoid函数饱和使梯度消失。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。
* Sigmoid函数的输出不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数。这将会导致梯度下降权重更新时出现z字型的下降。

### tanh

* $$tanh(x) = 2\sigma (2x) - 1$$
* 和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。

### ReLU

* $$f(x) = max(0, x)$$
* 优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用。据称这是由它的线性，非饱和的公式导致的。
* 优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。
* 缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。

### Leaky ReLU

* $$f(x) = 1(x < 0)(\alpha x) + 1(x >= 0)(x)$$
* Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x<0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。这个激活函数表现很不错，但是其效果并不是很稳定。

### Maxout

* $$max({w_1}^T x + {b_1}, {w_2}^T x + {b_2})$$
* Maxout是对ReLU和leaky ReLU的一般化归纳。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当w1b1=0时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。

## 25. 最优化

### 批量梯度下降 (Batch Gradient Descent)

* 程序重复地计算梯度然后对参数进行更新，采用整个训练集
* $$\theta = \theta - \eta \cdot {\nabla}_{\theta} J(\theta)$$
* 由于这种方法是在一次更新中，就对整个数据集计算梯度，所以计算起来非常慢，遇到很大量的数据集也会非常棘手，而且不能投入新数据实时更新模型
* Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值

### 随机梯度下降（Stochastic Gradient Descent)

* 和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本
* $$\theta = \theta - \eta \cdot {\nabla}_{\theta} J(\theta ; x^{(i)}; y^{(i)})$$
* SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处
* 稍微减小 learning rate，SGD 和 BGD 的收敛性是一样的

### 小批量数据梯度下降 (Mini-batch gradient descent)

* MBGD 每一次利用一小批样本，即 n 个样本进行计算。这样它可以降低参数更新时的方差，收敛更稳定。另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算
* $$\theta = \theta - \eta \cdot {\nabla}_{\theta} J(\theta ; x^{(i:i+n)}; y^{(i:i+n)})$$
* 缺点如下：
    - learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点
    - 这种方法是对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新
    - 对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error 是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里

### 梯度下降更新方法

* 普通更新
    - 算法
        ```Shell
        x += - learning_rate * dx
        ```
    - learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展
* Momentum
    - 算法
        ```shell
        v = mu * v - learning_rate * dx # 与速度融合
        x += v # 与位置融合
        ```
    - $$v_t = \gamma v_{t-1} + \eta {\nabla}_{\theta} J(\theta)$$ $$\theta = \theta - v_t$$
    - 通过动量更新，参数向量会在任何有持续梯度的方向上增加速度，并抑制震荡
    - 这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好
* Nesterov Momentum
    - 算法
        ```shell
        v_prev = v # 存储备份
        v = mu * v - learning_rate * dx # 速度更新保持不变
        x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
        ```
    - $$v_t = \gamma v_{t-1} + \eta {\nabla}_{\theta} J(\theta - \gamma v_{t-1})$$ $$\theta = \theta - v_t$$
    - 将未来的近似位置x + mu * v看做是“向前看”
    - 计算x + mu * v的梯度而不是“旧”位置x的梯度
    - NAG 会先在前一步的累积梯度上有一个大的跳跃，然后衡量一下梯度做一下修正，这种预期的更新可以避免我们走的太快，使 RNN 在很多任务上有更好的表现

### 学习率退火

* 如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。
* 随步数衰减：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。
* 指数衰减。公式为$\alpha = {\alpha}_0 e^{-kt}$，其中${\alpha}_0$、$k$是超参数，$t$是迭代次数
* $1/t$衰减的数学公式是$\alpha = {\alpha}_0 / (1 + kt)$，其中${\alpha}_0$、$k$是超参数，$t$是迭代次数

### 二阶方法

* $$x = x - [Hf(x)]^{-1} \nabla f(x)$$
* 直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进
* 没有超参数
* 很难运用到实际的深度学习应用中，因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间
* 即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本

### 逐参数适应学习率

* Adagrad
    - $${\theta}_{t+1,i} = {\theta}_{t,i} - {\eta \over \sqrt {G_{t,ii}+\epsilon}} \cdot g_{t,i}$$ where $$g_{t,i} = {\nabla}_{\theta} J(\theta_i)$$ if SGD, then $${\theta}_{t+1,i} = {\theta}_{t,i} - \eta \cdot g_{t,i}$$
    - 算法
        ```shell
        cache += dx**2
        x += - learning_rate * dx / (np.sqrt(cache) + eps)
        ```
    - 减少了学习率的手动调节
    - 变量cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和
    - 接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强
    - 在深度学习中单调的学习率被证明通常过于激进且过早停止学习
    - 它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小
* Adadelta

    - $$\Delta \theta_t = -{\eta \over \sqrt {E[g^2]_t + \epsilon}} g_t = -{\eta \over RMS[g]_t} g_t$$ where $$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma){g_t}^2$$ Change learning rate to $RMS[\Delta \theta]$, we have $$\Delta \theta_t = -{RMS[\Delta \theta]_{t-1} \over RMS[g]_t} g_t$$ $$\theta_{t+1} = \theta_t + \Delta \theta_t$$
    - 对 Adagrad 的改进

* RMSprop
    - $$E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma){g_t}^2 \quad(\gamma = 0.9)$$ $$\theta_{t+1} = \theta_{t} - {\eta \over \sqrt {E[g^2]_t + \epsilon}} g_t$$
    - 算法
        ```shell
        cache =  decay_rate * cache + (1 - decay_rate) * dx**2
        x += - learning_rate * dx / (np.sqrt(cache) + eps)
        ```
    - decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]
    - 基于梯度的大小来对每个权重的学习率进行修改
    - 其更新不会让学习率单调变小
* Adam
    - $$m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t$$ $$v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t$$ $$\hat{m} = {m_t \over 1 - {\beta_1}^t}$$ $$\hat{v} = {v_t \over 1 - {\beta_2}^t}$$ $$\theta_{t+1} = \theta_{t} - {\eta \over \sqrt {\hat{v}_t} + \epsilon} g_t$$ $$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10e-8$$
    - 算法
        ```shell
        m = beta1*m + (1-beta1)*dx
        v = beta2*v + (1-beta2)*(dx**2)
        x += - learning_rate * m / (np.sqrt(v) + eps)
        ```
    - 使用的是平滑版的梯度m，而不是用的原始梯度向量dx
    - 推荐的参数值eps=1e-8, beta1=0.9, beta2=0.999
    - 完整的Adam更新算法也包含了一个偏置（bias）矫正机制，因为m,v两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施
    
### 优化器选择

* 如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam
* RMSprop, Adadelta, Adam 在很多情况下的效果是相似的
* Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好
* SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点
* 如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法
* 在project中遇到采用 RMSprop 而不采用 Adam 的情况
